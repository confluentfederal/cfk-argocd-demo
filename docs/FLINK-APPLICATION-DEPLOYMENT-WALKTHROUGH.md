# Deploying a Custom Flink Application with ArgoCD

This walkthrough demonstrates deploying a custom Java Flink application to Confluent Platform on Kubernetes using ArgoCD GitOps. It includes real-world errors you'll encounter and how to resolve them.

## Prerequisites

- Kubernetes cluster with Confluent Platform deployed
- ArgoCD installed and configured
- Confluent Manager for Flink (CMF) running
- FlinkEnvironment (`flink-env`) created
- Docker Hub account for pushing images
- Maven installed locally

## Overview

We'll deploy a custom Kafka Streams processing application called `kafka-streaming-job` that:
- Reads from `flink-input` topic
- Processes messages
- Writes to `flink-output` topic

---

## Step 1: Create the Helm Chart Structure

First, create a reusable Flink application Helm chart:

```bash
mkdir -p charts/flink-application/templates
```

### Chart.yaml

```yaml
# charts/flink-application/Chart.yaml
apiVersion: v2
name: flink-application
description: Helm chart for deploying Flink applications on Confluent Platform
type: application
version: 1.0.0
appVersion: "1.20"
```

### Template: flink-application.yaml

```yaml
# charts/flink-application/templates/flink-application.yaml
apiVersion: platform.confluent.io/v1beta1
kind: FlinkApplication
metadata:
  name: {{ .Release.Name }}
  namespace: {{ .Values.namespace }}
spec:
  flinkEnvironment: {{ .Values.flinkEnvironment }}
  image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
  flinkVersion: {{ .Values.flinkVersion }}
  flinkConfiguration:
    {{- toYaml .Values.flinkConfiguration | nindent 4 }}
  serviceAccount: {{ .Values.serviceAccount }}
  jobManager:
    resource:
      {{- toYaml .Values.jobManager.resource | nindent 6 }}
  taskManager:
    resource:
      {{- toYaml .Values.taskManager.resource | nindent 6 }}
  job:
    jarURI: {{ .Values.job.jarURI }}
    {{- if .Values.job.entryClass }}
    entryClass: {{ .Values.job.entryClass }}
    {{- end }}
    state: {{ .Values.job.state }}
    parallelism: {{ .Values.job.parallelism }}
    upgradeMode: {{ .Values.job.upgradeMode | default "stateless" }}
    {{- if .Values.job.args }}
    args:
      {{- toYaml .Values.job.args | nindent 6 }}
    {{- end }}
  cmfRestClassRef:
    name: default
    namespace: {{ .Values.namespace }}
```

---

## Step 2: Create Values File for Your Application

```yaml
# charts/flink-application/values-kafka-streaming.yaml
namespace: confluent
flinkEnvironment: flink-env

image:
  repository: cstevenson954/java-flink
  tag: "0.0.1"

flinkVersion: v1_20
serviceAccount: flink

flinkConfiguration:
  taskmanager.numberOfTaskSlots: "2"

jobManager:
  resource:
    memory: "1024m"
    cpu: "0.5"

taskManager:
  resource:
    memory: "1024m"
    cpu: "0.5"

job:
  jarURI: local:///opt/flink/usrlib/java-flink-0.0.1.jar
  entryClass: com.example.javaflink.StreamingJob
  state: running
  parallelism: 1
  upgradeMode: stateless
  args:
    - "--bootstrap.servers=kafka.confluent.svc.cluster.local:9071"
    - "--input.topic=flink-input"
    - "--output.topic=flink-output"
```

---

## Step 3: Create ArgoCD Application

```yaml
# argocd/applications/flink-kafka-streaming.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: flink-kafka-streaming
  namespace: argocd
  labels:
    environment: prod
    component: flink
spec:
  project: confluent
  source:
    repoURL: https://github.com/your-org/your-repo.git
    targetRevision: main
    path: charts/flink-application
    helm:
      releaseName: kafka-streaming-job
      valueFiles:
        - values-kafka-streaming.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: confluent
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

---

## Step 4: Build and Push the Docker Image

```bash
cd /path/to/java-flink-project
mvn clean package -DskipTests
```

### ❌ ERROR 1: Compilation Fails - "cannot find symbol: variable log"

```
[ERROR] StreamingJob.java:[44,9] cannot find symbol
[ERROR]   symbol:   variable log
[ERROR]   location: class com.example.javaflink.StreamingJob
```

**Cause:** The project uses Lombok's `@Slf4j` annotation, but Lombok annotation processing isn't working.

**Fix:** Replace Lombok annotation with explicit SLF4J logger:

```java
// Before (using Lombok)
import lombok.extern.slf4j.Slf4j;

@Slf4j
public class StreamingJob {
    // log variable is generated by Lombok
}

// After (explicit logger)
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class StreamingJob {
    private static final Logger log = LoggerFactory.getLogger(StreamingJob.class);
}
```

Rebuild:
```bash
mvn clean package -DskipTests
```

---

## Step 5: Tag and Push Docker Image

```bash
docker tag java-flink:0.0.1 your-dockerhub/java-flink:0.0.1
docker push your-dockerhub/java-flink:0.0.1
```

---

## Step 6: Create Kafka Topics

Before deploying, ensure the topics exist:

```bash
kubectl exec -it kafka-0 -n confluent -- kafka-topics \
  --bootstrap-server localhost:9092 \
  --create --topic flink-input \
  --partitions 3 --replication-factor 3

kubectl exec -it kafka-0 -n confluent -- kafka-topics \
  --bootstrap-server localhost:9092 \
  --create --topic flink-output \
  --partitions 3 --replication-factor 3
```

---

## Step 7: Deploy with ArgoCD

```bash
# Commit and push your changes
git add -A
git commit -m "Add kafka-streaming Flink application"
git push origin main

# Apply the ArgoCD application
kubectl apply -f argocd/applications/flink-kafka-streaming.yaml
```

Check status:
```bash
kubectl get applications -n argocd | grep flink
```

---

## Step 8: Monitor Deployment

```bash
# Check FlinkApplication status
kubectl get flinkapplication -n confluent

# Check FlinkDeployment (created by CMF)
kubectl get flinkdeployments -n confluent

# Check pods
kubectl get pods -n confluent | grep kafka-streaming
```

### ❌ ERROR 2: Job Status FAILED - "NoClassDefFoundError: YAMLFactory"

```bash
kubectl describe flinkdeployment kafka-streaming-job -n confluent | grep -A5 "Error:"
```

```
Error: java.lang.NoClassDefFoundError: com/fasterxml/jackson/dataformat/yaml/YAMLFactory
  at com.example.javaflink.config.ConfigLoader.<clinit>(ConfigLoader.java:15)
```

**Cause:** The shaded JAR doesn't include the `jackson-dataformat-yaml` dependency because Jib runs BEFORE the shade plugin.

**Root Cause Analysis:**
In Maven, plugins execute in the order they're declared for the same phase. If `jib-maven-plugin` is declared before `maven-shade-plugin`, Jib copies the unshaded JAR (missing dependencies) to the Docker image.

**Fix:** Reorder plugins in `pom.xml` so shade runs first:

```xml
<build>
  <plugins>
    <!-- Other plugins... -->
    
    <!-- Maven Shade Plugin - MUST run before Jib -->
    <plugin>
      <groupId>org.apache.maven.plugins</groupId>
      <artifactId>maven-shade-plugin</artifactId>
      <version>3.6.0</version>
      <executions>
        <execution>
          <phase>package</phase>
          <goals>
            <goal>shade</goal>
          </goals>
          <!-- configuration... -->
        </execution>
      </executions>
    </plugin>

    <!-- Jib Plugin - runs AFTER shade plugin -->
    <plugin>
      <groupId>com.google.cloud.tools</groupId>
      <artifactId>jib-maven-plugin</artifactId>
      <version>3.4.6</version>
      <executions>
        <execution>
          <phase>package</phase>
          <goals>
            <goal>dockerBuild</goal>
          </goals>
        </execution>
      </executions>
      <!-- configuration... -->
    </plugin>
  </plugins>
</build>
```

Rebuild and push new image:
```bash
mvn clean package -DskipTests
docker tag java-flink:0.0.1 your-dockerhub/java-flink:0.0.2
docker push your-dockerhub/java-flink:0.0.2
```

Update `values-kafka-streaming.yaml`:
```yaml
image:
  tag: "0.0.2"
```

Commit, push, and sync:
```bash
git add -A && git commit -m "Fix: reorder Maven plugins" && git push
kubectl annotate application flink-kafka-streaming -n argocd argocd.argoproj.io/refresh=hard --overwrite
```

---

### ❌ ERROR 3: Job Status FAILED - "No resolvable bootstrap urls"

```bash
kubectl logs kafka-streaming-job-xxx -n confluent | grep -E "ERROR|Exception"
```

```
Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
```

**Cause:** The embedded `application.yml` has a placeholder bootstrap server (`broker:29092`) that doesn't resolve in Kubernetes. The app validates config BEFORE command-line args override it.

**Fix:** Update the embedded `src/main/resources/application.yml`:

```yaml
# Before
kafka:
  bootstrap-servers: "broker:29092"
  input-topic: "input-topic"
  output-topic: "output-topic"

# After
kafka:
  bootstrap-servers: "kafka.confluent.svc.cluster.local:9071"
  input-topic: "flink-input"
  output-topic: "flink-output"
  schema-registry-url: "http://schemaregistry.confluent.svc.cluster.local:8081"
```

Rebuild and push new image:
```bash
mvn clean package -DskipTests
docker tag java-flink:0.0.1 your-dockerhub/java-flink:0.0.3
docker push your-dockerhub/java-flink:0.0.3
```

Update `values-kafka-streaming.yaml`:
```yaml
image:
  tag: "0.0.3"
```

Commit, push, and sync:
```bash
git add -A && git commit -m "Fix: update Kafka bootstrap servers" && git push
kubectl annotate application flink-kafka-streaming -n argocd argocd.argoproj.io/refresh=hard --overwrite
```

---

## Step 9: Verify Successful Deployment

```bash
# Check FlinkDeployment status
kubectl get flinkdeployments -n confluent
```

Expected output:
```
NAME                    JOB STATUS   LIFECYCLE STATE
kafka-streaming-job     RUNNING      STABLE
```

```bash
# Check pods are running
kubectl get pods -n confluent | grep kafka-streaming
```

Expected output:
```
kafka-streaming-job-xxx-yyy                  1/1     Running   0   2m
kafka-streaming-job-taskmanager-1-1          1/1     Running   0   90s
```

---

## Troubleshooting Commands

### View FlinkDeployment Details
```bash
kubectl describe flinkdeployment kafka-streaming-job -n confluent
```

### View Job Manager Logs
```bash
kubectl logs -l app=kafka-streaming-job,component=jobmanager -n confluent
```

### View Task Manager Logs
```bash
kubectl logs -l app=kafka-streaming-job,component=taskmanager -n confluent
```

### Check CMF Logs
```bash
kubectl logs deployment/confluent-manager-for-apache-flink -n confluent --tail=50
```

### Force ArgoCD Sync
```bash
kubectl annotate application flink-kafka-streaming -n argocd \
  argocd.argoproj.io/refresh=hard --overwrite
```

### Delete and Recreate FlinkApplication
```bash
kubectl delete flinkapplication kafka-streaming-job -n confluent
# ArgoCD will recreate it automatically due to selfHeal: true
```

---

## Summary of Errors and Fixes

| Error | Symptom | Root Cause | Fix |
|-------|---------|------------|-----|
| Compilation failure | `cannot find symbol: variable log` | Lombok annotation processing not working | Replace `@Slf4j` with explicit SLF4J logger |
| NoClassDefFoundError | `YAMLFactory` class not found at runtime | Jib copies JAR before shade plugin runs | Reorder plugins in pom.xml: shade before Jib |
| Kafka connection failure | `No resolvable bootstrap urls` | Embedded config has placeholder values | Update `application.yml` with K8s DNS names |

---

## Key Takeaways

1. **Maven plugin order matters** - Plugins in the same phase execute in declaration order
2. **Fat JAR must include all dependencies** - Verify with `jar tf your-app.jar | grep jackson`
3. **Config validation happens before overrides** - Embedded defaults must be valid
4. **ArgoCD syncs from Git** - Always commit and push before expecting changes
5. **CMF creates FlinkDeployments** - FlinkApplication → CMF → FlinkDeployment → Pods
